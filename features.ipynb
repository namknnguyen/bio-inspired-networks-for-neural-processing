{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import mne\n",
    "from scipy.signal import butter, filtfilt\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    filename='pipeline.log',\n",
    "    filemode='w',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "\n",
    "edf_dir = 'raw_data/chb04/'\n",
    "summary_path = 'raw_data/chb04/chb04-summary.txt'\n",
    "output_dir = 'output2/chb04/'\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "SAMPLING_RATE = 256\n",
    "SEGMENT_DURATION = 30  \n",
    "WINDOW_DURATION = 3  \n",
    "NUM_WINDOWS = SEGMENT_DURATION // WINDOW_DURATION  \n",
    "PRE_ICTAL_DURATION = 1800\n",
    "POST_ICTAL_DURATION = 1800\n",
    "EXCLUDE_WINDOW = 7200\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.8 \n",
    "\n",
    "def parse_time(time_str):\n",
    "    h, m, s = map(int, time_str.split(':'))\n",
    "    if h == 24:\n",
    "        h = 0\n",
    "    return h * 3600 + m * 60 + s\n",
    "\n",
    "def parse_summary(summary_path):\n",
    "    file_info = {}\n",
    "    day = 0\n",
    "    prev_start = -1\n",
    "    with open(summary_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        if lines[i].startswith('File Name:'):\n",
    "            fname = lines[i].split(':')[1].strip()\n",
    "            i += 1\n",
    "            start_time_line = lines[i].strip()\n",
    "            start_time_str = ':'.join(start_time_line.split(':')[1:]).strip()\n",
    "            start_time_parsed = parse_time(start_time_str)\n",
    "            if prev_start != -1 and start_time_parsed < prev_start:\n",
    "                day += 1\n",
    "            global_start = day * 86400 + start_time_parsed\n",
    "            file_info[fname] = {'global_start': global_start, 'seizures': []}\n",
    "            prev_start = start_time_parsed\n",
    "            i += 1  \n",
    "            i += 1  \n",
    "            num_seizures = int(lines[i].split(':')[1].strip())\n",
    "            if num_seizures > 0:\n",
    "                for j in range(num_seizures):\n",
    "                    i += 1\n",
    "                    seizure_start = int(lines[i].split(':')[1].split()[0])\n",
    "                    i += 1\n",
    "                    seizure_end = int(lines[i].split(':')[1].split()[0])\n",
    "                    global_seizure_start = global_start + seizure_start\n",
    "                    global_seizure_end = global_start + seizure_end\n",
    "                    file_info[fname]['seizures'].append((global_seizure_start, global_seizure_end))\n",
    "            else:\n",
    "                i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return file_info\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut=0.1, highcut=127, fs=256, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, data, axis=-1)\n",
    "\n",
    "def extract_spectral_features(segment, fs=256, bands={'delta': (0.1, 4), 'theta': (4, 8), 'alpha': (8, 12), 'beta': (12, 30), 'gamma': (30, 127)}):\n",
    "    n_samples = segment.shape[-1]\n",
    "    freqs = np.fft.rfftfreq(n_samples, 1/fs)\n",
    "    fft_vals = np.fft.rfft(segment, axis=-1)\n",
    "    features = []\n",
    "    for band in bands:\n",
    "        low, high = bands[band]\n",
    "        idx = np.where((freqs >= low) & (freqs <= high))[0]\n",
    "        band_fft = fft_vals[:, idx]\n",
    "        power = np.mean(np.abs(band_fft)**2, axis=-1)\n",
    "        mean_amp = np.mean(np.abs(band_fft), axis=-1)\n",
    "        avg_feature = (power + mean_amp) / 2\n",
    "        features.append(avg_feature)\n",
    "    features = np.stack(features, axis=1)  \n",
    "    return features\n",
    "\n",
    "def get_common_channels(file_names, edf_dir):\n",
    "    channel_sets = []\n",
    "    for file_name in file_names:\n",
    "        raw = mne.io.read_raw_edf(os.path.join(edf_dir, file_name), preload=False, verbose=False)\n",
    "        channel_sets.append(set(raw.ch_names))\n",
    "    common_channels = set.intersection(*channel_sets)\n",
    "    return sorted(list(common_channels)) \n",
    "\n",
    "def extract_segments(file_name, file_info, common_channels):\n",
    "    raw = mne.io.read_raw_edf(os.path.join(edf_dir, file_name), preload=False, verbose=False)\n",
    "    raw.pick_channels(common_channels)  \n",
    "    logging.info(f\"After selection, file {file_name} has {raw.info['nchan']} channels\")\n",
    "    duration = raw.times[-1]\n",
    "    fs = raw.info['sfreq']\n",
    "    assert fs == SAMPLING_RATE, f\"Sampling rate is {fs}, expected {SAMPLING_RATE}\"\n",
    "    global_start = file_info[file_name]['global_start']\n",
    "    sequence_id = hash(file_name) % 10000 \n",
    "\n",
    "    local_seizures = file_info[file_name]['seizures']\n",
    "    ictal_periods = [(s, e) for s, e in local_seizures]\n",
    "    post_ictal_periods = [(e, e + POST_ICTAL_DURATION) for s, e in local_seizures]\n",
    "\n",
    "    all_seizures = [seizure for fname in file_info for seizure in file_info[fname]['seizures']]\n",
    "    exclude_windows = [(s - EXCLUDE_WINDOW, e + EXCLUDE_WINDOW) for s, e in all_seizures]\n",
    "    pre_ictal_periods = [(s - PRE_ICTAL_DURATION, s) for s, e in all_seizures]\n",
    "\n",
    "    step_size = SEGMENT_DURATION\n",
    "    for t in np.arange(0, duration - SEGMENT_DURATION + 1, step_size):\n",
    "        t_end = t + SEGMENT_DURATION\n",
    "        absolute_t = global_start + t\n",
    "        absolute_t_end = absolute_t + SEGMENT_DURATION\n",
    "\n",
    "        if any(s <= absolute_t_end and e >= absolute_t for s, e in ictal_periods) or \\\n",
    "           any(s <= absolute_t_end and e >= absolute_t for s, e in post_ictal_periods):\n",
    "            continue\n",
    "\n",
    "        in_pre_ictal = any(p <= absolute_t < s for p, s in pre_ictal_periods)\n",
    "        in_exclude_window = any(s <= absolute_t_end and e >= absolute_t for s, e in exclude_windows)\n",
    "\n",
    "        if in_pre_ictal:\n",
    "            label = 1\n",
    "        elif in_exclude_window:\n",
    "            continue\n",
    "        else:\n",
    "            label = 0\n",
    "\n",
    "        window_index = int(t / step_size)\n",
    "        start_sample = int(t * fs)\n",
    "        stop_sample = int(t_end * fs)\n",
    "        segment_data = raw.get_data(start=start_sample, stop=stop_sample)  \n",
    "\n",
    "\n",
    "        window_samples = int(WINDOW_DURATION * fs) \n",
    "        window_features = []\n",
    "        for w in range(NUM_WINDOWS):\n",
    "            w_start = w * window_samples\n",
    "            w_end = (w + 1) * window_samples\n",
    "            window_data = segment_data[:, w_start:w_end]  \n",
    "            filtered_data = butter_bandpass_filter(window_data)\n",
    "            features = extract_spectral_features(filtered_data)  \n",
    "            features = features.transpose(1, 0) \n",
    "            features = np.mean(features, axis=0) \n",
    "            window_features.append(features)\n",
    "        window_features = np.stack(window_features, axis=0) \n",
    "\n",
    "        yield (file_name, t, t_end, window_features, label, sequence_id, window_index)\n",
    "\n",
    "def process_and_save_all_files(file_names, file_info, patient_id, common_channels):\n",
    "    all_segments = []\n",
    "    for file_name in file_names:\n",
    "        logging.info(f\"Collecting segments from file: {file_name}\")\n",
    "        segment_generator = extract_segments(file_name, file_info, common_channels)\n",
    "        for fname, start, end, features, label, seq_id, win_idx in segment_generator:\n",
    "            global_start = file_info[fname]['global_start'] + start\n",
    "            all_segments.append({\n",
    "                'file_name': fname,\n",
    "                'start': start,\n",
    "                'end': end,\n",
    "                'features': features, \n",
    "                'label': label,\n",
    "                'sequence_id': seq_id,\n",
    "                'window_index': win_idx,\n",
    "                'global_start': global_start\n",
    "            })\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(all_segments)\n",
    "\n",
    "\n",
    "    total_segments = len(all_segments)\n",
    "    train_size = int(total_segments * TRAIN_TEST_SPLIT_RATIO)\n",
    "    train_segments = all_segments[:train_size]\n",
    "    test_segments = all_segments[train_size:]\n",
    "\n",
    "    test_labels = [seg['label'] for seg in test_segments]\n",
    "    n_pre_ictal_test = sum(1 for label in test_labels if label == 1)\n",
    "    n_inter_ictal_test = len(test_labels) - n_pre_ictal_test\n",
    "    logging.info(f\"Test segments: {len(test_segments)} (Pre-ictal: {n_pre_ictal_test}, Inter-ictal: {n_inter_ictal_test})\")\n",
    "    logging.info(f\"Total segments: {total_segments}, Train: {len(train_segments)}, Test: {len(test_segments)}\")\n",
    "\n",
    "    train_features_list = []\n",
    "    train_labels_list = []\n",
    "    train_sequence_ids_list = []\n",
    "    train_window_indices_list = []\n",
    "    train_csv_data = []\n",
    "\n",
    "    for segment in train_segments:\n",
    "        try:\n",
    "            feature_tensor = torch.from_numpy(segment['features']).float().unsqueeze(0)  \n",
    "            train_features_list.append(feature_tensor)\n",
    "            train_labels_list.append(segment['label'])\n",
    "            train_sequence_ids_list.append(segment['sequence_id'])\n",
    "            train_window_indices_list.append(segment['window_index'])\n",
    "            train_csv_data.append({\n",
    "                'patient_id': patient_id,\n",
    "                'file_name': segment['file_name'],\n",
    "                'start_time': segment['start'],\n",
    "                'end_time': segment['end'],\n",
    "                'label': segment['label']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing train segment in {segment['file_name']}: {str(e)}\")\n",
    "        gc.collect()\n",
    "\n",
    "    test_features_list = []\n",
    "    test_labels_list = []\n",
    "    test_sequence_ids_list = []\n",
    "    test_window_indices_list = []\n",
    "    test_csv_data = []\n",
    "\n",
    "    for segment in test_segments:\n",
    "        try:\n",
    "            feature_tensor = torch.from_numpy(segment['features']).float().unsqueeze(0) \n",
    "            test_features_list.append(feature_tensor)\n",
    "            test_labels_list.append(segment['label'])\n",
    "            test_sequence_ids_list.append(segment['sequence_id'])\n",
    "            test_window_indices_list.append(segment['window_index'])\n",
    "            test_csv_data.append({\n",
    "                'patient_id': patient_id,\n",
    "                'file_name': segment['file_name'],\n",
    "                'start_time': segment['start'],\n",
    "                'end_time': segment['end'],\n",
    "                'label': segment['label']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing test segment in {segment['file_name']}: {str(e)}\")\n",
    "        gc.collect()\n",
    "\n",
    "    if train_features_list:\n",
    "        features_tensor = torch.cat(train_features_list, dim=0) \n",
    "        mean = features_tensor.mean(dim=(0, 1), keepdim=True)\n",
    "        std = features_tensor.std(dim=(0, 1), keepdim=True) + 1e-6\n",
    "        features_tensor = (features_tensor - mean) / std\n",
    "        labels_tensor = torch.tensor(train_labels_list, dtype=torch.long)\n",
    "        sequence_ids_tensor = torch.tensor(train_sequence_ids_list, dtype=torch.long)\n",
    "        window_indices_tensor = torch.tensor(train_window_indices_list, dtype=torch.long)\n",
    "        torch.save({\n",
    "            'features': features_tensor,\n",
    "            'labels': labels_tensor,\n",
    "            'sequence_ids': sequence_ids_tensor,\n",
    "            'window_indices': window_indices_tensor\n",
    "        }, os.path.join(output_dir, f'train_fold1_{patient_id}.pt'))\n",
    "        pd.DataFrame(train_csv_data).to_csv(os.path.join(output_dir, f'train_fold1_{patient_id}.csv'), index=False)\n",
    "        logging.info(f\"Saved train_fold1_{patient_id} with {len(train_features_list)} segments.\")\n",
    "    else:\n",
    "        logging.warning(f\"No train segments to save for {patient_id}.\")\n",
    "\n",
    "    if test_features_list:\n",
    "        features_tensor = torch.cat(test_features_list, dim=0)  \n",
    "        features_tensor = (features_tensor - mean) / std\n",
    "        labels_tensor = torch.tensor(test_labels_list, dtype=torch.long)\n",
    "        sequence_ids_tensor = torch.tensor(test_sequence_ids_list, dtype=torch.long)\n",
    "        window_indices_tensor = torch.tensor(test_window_indices_list, dtype=torch.long)\n",
    "        torch.save({\n",
    "            'features': features_tensor,\n",
    "            'labels': labels_tensor,\n",
    "            'sequence_ids': sequence_ids_tensor,\n",
    "            'window_indices': window_indices_tensor\n",
    "        }, os.path.join(output_dir, f'test_fold1_{patient_id}.pt'))\n",
    "        pd.DataFrame(test_csv_data).to_csv(os.path.join(output_dir, f'test_fold1_{patient_id}.csv'), index=False)\n",
    "        logging.info(f\"Saved test_fold1_{patient_id} with {len(test_features_list)} segments.\")\n",
    "    else:\n",
    "        logging.warning(f\"No test segments to save for {patient_id}.\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "def main():\n",
    "    total, used, free = shutil.disk_usage(output_dir)\n",
    "    free_gb = free / (2**30)\n",
    "    if free_gb < 10:\n",
    "        logging.warning(f\"Low disk space: {free_gb:.2f} GB free. May cause write failures.\")\n",
    "\n",
    "    logging.info(\"Parsing summary...\")\n",
    "    file_info = parse_summary(summary_path)\n",
    "    files = list(file_info.keys())\n",
    "    patient_id = 'chb04'\n",
    "\n",
    "    common_channels = get_common_channels(files, edf_dir)\n",
    "    logging.info(f\"Common channels for {patient_id}: {common_channels} (Count: {len(common_channels)})\")\n",
    "\n",
    "    logging.info(f\"Processing all files for patient {patient_id}\")\n",
    "    process_and_save_all_files(files, file_info, patient_id, common_channels)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
